# HigherSelf Network Server - Production Override Configuration
# This file provides production-specific overrides for docker-compose.unified.yml
# Use with: docker-compose -f docker-compose.unified.yml -f docker-compose.prod.yml up

version: '3.8'

services:
  # Production configuration for main application server
  higherself-server:
    image: thehigherselfnetworkserver:${IMAGE_TAG:-latest}  # Use pre-built image
    environment:
      - DEBUG=false
      - LOG_LEVEL=INFO
      - SERVER_RELOAD=false
      - TESTING=false
      - ENVIRONMENT=production
      - MULTI_ENTITY_MODE=true
      
      # Production database settings
      - MONGODB_DB_NAME=higherself_production
      - REDIS_DB=0
      
      # Production feature flags
      - ENABLE_DEBUG_ENDPOINTS=false
      - ENABLE_EXPERIMENTAL_FEATURES=false
      - ENABLE_PERFORMANCE_TESTING=false
      - DISABLE_WEBHOOKS=false
      
      # Production secrets management
      - SECRETS_BACKEND=aws_secrets_manager
      - SECRETS_FALLBACK_BACKEND=vault
      - ENABLE_SECRETS_ROTATION=true
      - ENABLE_ENCRYPTION_AT_REST=true
      - ENABLE_ENCRYPTION_IN_TRANSIT=true
      
      # Production monitoring and logging
      - ENABLE_AUDIT_LOGGING=true
      - ENABLE_DETAILED_MONITORING=true
      - ENABLE_ALERTING=true
    ports: []  # No direct port exposure - only through nginx
    volumes:
      # Production volumes (no source code mounting)
      - ./logs/production:/app/logs
      - ./data/production:/app/data
      - ./config:/app/config:ro
      - ./backups:/app/backups
    secrets:
      - notion_api_token
      - openai_api_key
      - mongodb_password
      - redis_password
      - jwt_secret_key
      - webhook_secret
      - vault_token
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    command: >
      gunicorn main:app 
      --bind 0.0.0.0:8000 
      --workers 4 
      --worker-class uvicorn.workers.UvicornWorker
      --timeout 120
      --keepalive 5
      --max-requests 1000
      --max-requests-jitter 100
      --preload
      --log-level info

  # Production worker configuration
  celery-worker:
    environment:
      - CELERY_WORKER_CONCURRENCY=8
      - CELERY_WORKER_LOGLEVEL=info
      - ENVIRONMENT=production
    volumes:
      - ./logs/production:/app/logs
      - ./data/production:/app/data
      - ./config:/app/config:ro
    secrets:
      - notion_api_token
      - openai_api_key
      - mongodb_password
      - redis_password
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 1.5G
        reservations:
          cpus: '0.5'
          memory: 512M
      replicas: 2  # Multiple worker instances
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Production scheduler
  celery-beat:
    environment:
      - ENVIRONMENT=production
    volumes:
      - ./logs/production:/app/logs
      - ./data/production:/app/data
      - ./config:/app/config:ro
    secrets:
      - notion_api_token
      - openai_api_key
      - mongodb_password
      - redis_password
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "3"

  # Production MongoDB with replica set
  mongodb:
    ports: []  # No external port exposure
    environment:
      - MONGO_INITDB_DATABASE=higherself_production
    volumes:
      - mongodb_data_production:/data/db
      - ./deployment/mongodb/production:/docker-entrypoint-initdb.d:ro
      - ./logs/production/mongodb:/var/log/mongodb
      - ./backups/mongodb:/backups
    command:
      - "--auth"
      - "--bind_ip_all"
      - "--logpath"
      - "/var/log/mongodb/mongod.log"
      - "--logappend"
      - "--wiredTigerCacheSizeGB"
      - "4"  # Increased cache for production
      - "--replSet"
      - "rs0"
      - "--oplogSize"
      - "2048"
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 6G
        reservations:
          cpus: '1.0'
          memory: 4G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  # Production Redis with authentication
  redis:
    ports: []  # No external port exposure
    volumes:
      - redis_data_production:/data
      - ./deployment/redis/production.conf:/usr/local/etc/redis/redis.conf:ro
      - ./logs/production/redis:/var/log/redis
    command: redis-server /usr/local/etc/redis/redis.conf --requirepass ${REDIS_PASSWORD}
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "3"

  # Production Consul cluster
  consul:
    ports: []  # No external port exposure
    volumes:
      - consul_data_production:/consul/data
      - ./logs/production/consul:/consul/logs
      - ./deployment/consul/production.hcl:/consul/config/consul.hcl:ro
    command: >
      agent -server -ui -client=0.0.0.0 -bootstrap-expect=3
      -data-dir=/consul/data -config-file=/consul/config/consul.hcl
      -log-file=/consul/logs/consul.log -log-level=INFO
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
      replicas: 3  # Consul cluster
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "3"

  # Production Prometheus with long retention
  prometheus:
    ports: []  # Access through nginx only
    volumes:
      - ./deployment/prometheus/production.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data_production:/prometheus
      - ./logs/production/prometheus:/var/log/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=90d"  # Long retention for production
      - "--storage.tsdb.retention.size=50GB"
      - "--web.enable-lifecycle"
      - "--web.enable-admin-api"
      - "--log.level=info"
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 2G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Production Grafana
  grafana:
    ports: []  # Access through nginx only
    volumes:
      - ./deployment/grafana/production:/etc/grafana/provisioning:ro
      - grafana_data_production:/var/lib/grafana
      - ./logs/production/grafana:/var/log/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_LOG_LEVEL=info
      - GF_SECURITY_DISABLE_GRAVATAR=true
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_ANALYTICS_REPORTING_ENABLED=false
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "3"

  # Production Nginx with SSL termination
  nginx:
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./deployment/nginx/production.conf:/etc/nginx/conf.d/default.conf:ro
      - ./deployment/nginx/ssl:/etc/nginx/ssl:ro
      - ./logs/production/nginx:/var/log/nginx
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  # Production Vault cluster
  vault:
    ports: []  # Internal access only
    volumes:
      - vault_data_production:/vault/data
      - ./deployment/vault/production.hcl:/vault/config/vault.hcl:ro
      - ./logs/production/vault:/vault/logs
    environment:
      - VAULT_LOG_LEVEL=info
    command: vault server -config=/vault/config/vault.hcl
    cap_add:
      - IPC_LOCK
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
      replicas: 3  # Vault cluster
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "3"

# Production volumes with specific drivers and options
volumes:
  mongodb_data_production:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/higherself/data/mongodb
  redis_data_production:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/higherself/data/redis
  consul_data_production:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/higherself/data/consul
  prometheus_data_production:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/higherself/data/prometheus
  grafana_data_production:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/higherself/data/grafana
  vault_data_production:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/higherself/data/vault

# Production network with custom subnet
networks:
  higherself_network:
    driver: bridge
    name: higherself-network-production
    ipam:
      driver: default
      config:
        - subnet: 172.21.0.0/16

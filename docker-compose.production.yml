version: '3.8'

services:
  # Main Higher Self Network Server Application
  higherself-server:
    build:
      context: .
      dockerfile: Dockerfile.production
      args:
        - BUILD_ENV=production
    container_name: higherself-server-prod
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
      - REDIS_URI=${REDIS_URI}
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - REDIS_SSL=${REDIS_SSL:-true}
      - REDIS_TIMEOUT=${REDIS_TIMEOUT:-10}
      - REDIS_MAX_CONNECTIONS=${REDIS_MAX_CONNECTIONS:-20}
      - NOTION_API_TOKEN=${NOTION_API_TOKEN}
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_API_KEY=${SUPABASE_API_KEY}
      - SUPABASE_SERVICE_ROLE_KEY=${SUPABASE_SERVICE_ROLE_KEY}
      - MONGODB_URI=${MONGODB_URI}
      - NEO4J_URI=${NEO4J_URI}
      - NEO4J_USER=${NEO4J_USER}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - HUGGINGFACE_API_KEY=${HUGGINGFACE_API_KEY}
    volumes:
      - app_logs:/app/logs
      - app_cache:/app/cache
      - app_data:/app/data
    networks:
      - higherself-network
      - monitoring-network
    depends_on:
      mongodb:
        condition: service_healthy
      neo4j:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.higherself.rule=Host(`api.higherself.network`)"
      - "traefik.http.routers.higherself.tls=true"
      - "traefik.http.routers.higherself.tls.certresolver=letsencrypt"

  # Celery Worker for Background Tasks
  celery-worker:
    build:
      context: .
      dockerfile: Dockerfile.production
    container_name: higherself-celery-worker
    restart: unless-stopped
    command: celery -A services.task_queue_service worker --loglevel=info --concurrency=4
    environment:
      - ENVIRONMENT=production
      - REDIS_URI=${REDIS_URI}
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - MONGODB_URI=${MONGODB_URI}
      - NOTION_API_TOKEN=${NOTION_API_TOKEN}
    volumes:
      - app_logs:/app/logs
      - app_cache:/app/cache
    networks:
      - higherself-network
    depends_on:
      - higherself-server
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # MongoDB Database
  mongodb:
    image: mongo:7.0
    container_name: higherself-mongodb
    restart: unless-stopped
    ports:
      - "27017:27017"
    environment:
      - MONGO_INITDB_ROOT_USERNAME=${MONGODB_USERNAME}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGODB_PASSWORD}
      - MONGO_INITDB_DATABASE=${MONGODB_DB_NAME}
    volumes:
      - mongodb_data:/data/db
      - mongodb_config:/data/configdb
      - ./scripts/mongodb-init.js:/docker-entrypoint-initdb.d/init.js:ro
    networks:
      - higherself-network
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # Neo4j Knowledge Graph Database
  neo4j:
    image: neo4j:5.15-community
    container_name: higherself-neo4j
    restart: unless-stopped
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
      - NEO4J_dbms_memory_heap_initial_size=1G
      - NEO4J_dbms_memory_heap_max_size=2G
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_import:/var/lib/neo4j/import
      - neo4j_plugins:/plugins
    networks:
      - higherself-network
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "${NEO4J_USER}", "-p", "${NEO4J_PASSWORD}", "RETURN 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 3G
        reservations:
          cpus: '0.5'
          memory: 1G

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: higherself-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - nginx_logs:/var/log/nginx
    networks:
      - higherself-network
    depends_on:
      - higherself-server
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Prometheus Monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: higherself-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks:
      - monitoring-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Grafana Dashboard
  grafana:
    image: grafana/grafana:latest
    container_name: higherself-grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    networks:
      - monitoring-network
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis Health Check Service (for Redis Cloud connectivity)
  redis-health-check:
    image: redis:alpine
    container_name: higherself-redis-health
    restart: "no"
    command: >
      sh -c "
        redis-cli -u ${REDIS_URI} --tls --cert /etc/ssl/certs/redis.crt --key /etc/ssl/private/redis.key ping &&
        echo 'Redis Cloud connection successful'
      "
    environment:
      - REDIS_URI=${REDIS_URI}
    networks:
      - higherself-network

networks:
  higherself-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
  monitoring-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/16

volumes:
  # Application volumes
  app_logs:
    driver: local
  app_cache:
    driver: local
  app_data:
    driver: local
  
  # Database volumes
  mongodb_data:
    driver: local
  mongodb_config:
    driver: local
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
  neo4j_import:
    driver: local
  neo4j_plugins:
    driver: local
  
  # Monitoring volumes
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  
  # Nginx volumes
  nginx_logs:
    driver: local

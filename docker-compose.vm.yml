# ======================================================
# THE HIGHERSELF NETWORK SERVER - VM PRODUCTION DEPLOYMENT
# MULTI-BUSINESS ENTITY CONFIGURATION
# ======================================================

version: '3.8'

services:
  # Main application server - Multi-entity production
  higherself-server:
    image: thehigherselfnetworkserver:vm-production
    build:
      context: .
      dockerfile: ./Dockerfile
      args:
        ENVIRONMENT: production
    container_name: higherself-server-vm
    ports:
      - "80:8000"      # HTTP traffic
      - "443:8443"     # HTTPS traffic (if SSL configured)
    environment:
      - RUNNING_IN_CONTAINER=true
      - PYTHONUNBUFFERED=1
      - ENVIRONMENT=production
      - VM_DEPLOYMENT=true
      - MULTI_ENTITY_MODE=true
      - MONGODB_URI=mongodb://higherself_user:${MONGODB_PASSWORD}@mongodb-vm:27017/higherself_production
      - REDIS_URI=redis://:${REDIS_PASSWORD}@redis-vm:6379/0
      - CONSUL_HTTP_ADDR=consul-vm:8500
    env_file:
      - .env.vm.production
    volumes:
      - ./logs/vm:/app/logs
      - ./data/vm:/app/data
      - ./config:/app/config:ro
      - ./ssl:/app/ssl:ro  # SSL certificates if using HTTPS
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 90s
    restart: unless-stopped
    depends_on:
      mongodb-vm:
        condition: service_healthy
      redis-vm:
        condition: service_healthy
      consul-vm:
        condition: service_healthy
    networks:
      - higherself-vm-network
    labels:
      - "prometheus.scrape=true"
      - "prometheus.port=8000"
      - "prometheus.path=/metrics"
      - "vm.deployment=production"
      - "business.entities=the_7_space,am_consulting,higherself_core"
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  # MongoDB - Production instance with all business entities
  mongodb-vm:
    image: mongo:6.0
    container_name: higherself-mongodb-vm
    ports:
      - "27017:27017"
    volumes:
      - mongodb_vm_data:/data/db
      - ./deployment/mongodb/vm:/docker-entrypoint-initdb.d
      - ./logs/vm/mongodb:/var/log/mongodb
      - ./backups/mongodb:/backups
    environment:
      - MONGO_INITDB_ROOT_USERNAME=admin
      - MONGO_INITDB_ROOT_PASSWORD=${MONGODB_ROOT_PASSWORD}
      - MONGO_INITDB_DATABASE=higherself_production
      - MONGO_APP_USER=higherself_user
      - MONGO_APP_PASSWORD=${MONGODB_PASSWORD}
    command: >
      mongod
      --auth
      --bind_ip_all
      --logpath /var/log/mongodb/mongod.log
      --logappend
      --oplogSize 1024
    healthcheck:
      test: |
        echo 'try {
          db.runCommand("ping").ok ? 0 : 2
        } catch(err) {
          print("MongoDB health check failed: " + err);
          2
        }' | mongosh localhost:27017/higherself_production --quiet
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - higherself-vm-network
    labels:
      - "vm.service=database"
      - "backup.enabled=true"
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 3G
        reservations:
          cpus: '0.5'
          memory: 1G

  # Redis - Production instance for caching and job queuing
  redis-vm:
    image: redis:7-alpine
    container_name: higherself-redis-vm
    ports:
      - "6379:6379"
    volumes:
      - redis_vm_data:/data
      - ./logs/vm/redis:/var/log/redis
      - ./config/redis/vm:/usr/local/etc/redis
    command: >
      redis-server 
      /usr/local/etc/redis/redis.conf
      --requirepass ${REDIS_PASSWORD}
      --appendonly yes
      --appendfsync everysec
      --maxmemory 1gb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    networks:
      - higherself-vm-network
    labels:
      - "vm.service=cache"
      - "backup.enabled=true"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 512M

  # Consul - Service discovery and configuration
  consul-vm:
    image: consul:1.15
    container_name: higherself-consul-vm
    ports:
      - "8500:8500"
      - "8600:8600/udp"
    volumes:
      - consul_vm_data:/consul/data
      - ./deployment/consul/vm:/consul/config
      - ./logs/vm/consul:/consul/logs
    environment:
      - CONSUL_BIND_INTERFACE=eth0
      - CONSUL_CLIENT_INTERFACE=eth0
    command: >
      consul agent 
      -server 
      -bootstrap-expect=1 
      -datacenter=higherself-vm 
      -data-dir=/consul/data 
      -config-dir=/consul/config 
      -ui-content-path=/consul/ 
      -log-level=INFO 
      -client=0.0.0.0
      -retry-join=consul-vm
    healthcheck:
      test: ["CMD", "consul", "members"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    restart: unless-stopped
    networks:
      - higherself-vm-network
    labels:
      - "vm.service=discovery"
    deploy:
      resources:
        limits:
          cpus: '0.3'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 256M

  # Prometheus - Metrics collection for production monitoring
  prometheus-vm:
    image: prom/prometheus:latest
    container_name: higherself-prometheus-vm
    ports:
      - "9090:9090"
    volumes:
      - ./deployment/prometheus/vm:/etc/prometheus
      - prometheus_vm_data:/prometheus
      - ./logs/vm/prometheus:/var/log/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.external-url=http://YOUR_VM_IP:9090'
      - '--storage.tsdb.max-block-duration=2h'
      - '--storage.tsdb.min-block-duration=2h'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    restart: unless-stopped
    depends_on:
      - higherself-server
    networks:
      - higherself-vm-network
    labels:
      - "vm.service=monitoring"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 512M

  # Grafana - Production dashboard and visualization
  grafana-vm:
    image: grafana/grafana:latest
    container_name: higherself-grafana-vm
    ports:
      - "3000:3000"
    volumes:
      - ./deployment/grafana/vm:/etc/grafana/provisioning
      - grafana_vm_data:/var/lib/grafana
      - ./logs/vm/grafana:/var/log/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_SERVER_ROOT_URL=http://YOUR_VM_IP:3000/
      - GF_SERVER_SERVE_FROM_SUB_PATH=false
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource,grafana-worldmap-panel
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_USERS_DEFAULT_THEME=dark
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
      - GF_SECURITY_DISABLE_GRAVATAR=true
      - GF_SMTP_ENABLED=true
      - GF_SMTP_HOST=${SMTP_HOST}:${SMTP_PORT}
      - GF_SMTP_USER=${SMTP_USER}
      - GF_SMTP_PASSWORD=${SMTP_PASSWORD}
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    restart: unless-stopped
    depends_on:
      - prometheus-vm
    networks:
      - higherself-vm-network
    labels:
      - "vm.service=dashboard"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 512M

  # Nginx - Reverse proxy and load balancer
  nginx-vm:
    image: nginx:alpine
    container_name: higherself-nginx-vm
    ports:
      - "8080:80"
      - "8443:443"
    volumes:
      - ./deployment/nginx/vm:/etc/nginx/conf.d
      - ./ssl:/etc/nginx/ssl:ro
      - ./logs/vm/nginx:/var/log/nginx
    depends_on:
      - higherself-server
    restart: unless-stopped
    networks:
      - higherself-vm-network
    labels:
      - "vm.service=proxy"
    deploy:
      resources:
        limits:
          cpus: '0.3'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M

  # Backup service for automated backups
  backup-vm:
    image: alpine:latest
    container_name: higherself-backup-vm
    volumes:
      - ./backups:/backups
      - ./scripts/backup:/scripts
      - mongodb_vm_data:/data/mongodb:ro
      - redis_vm_data:/data/redis:ro
      - ./logs/vm:/logs:ro
    environment:
      - BACKUP_SCHEDULE=${BACKUP_SCHEDULE:-0 2 * * *}
      - BACKUP_RETENTION_DAYS=${BACKUP_RETENTION_DAYS:-30}
      - MONGODB_URI=mongodb://higherself_user:${MONGODB_PASSWORD}@mongodb-vm:27017/higherself_production
    command: >
      sh -c "
        apk add --no-cache mongodb-tools redis curl &&
        crond -f -d 8
      "
    restart: unless-stopped
    depends_on:
      - mongodb-vm
      - redis-vm
    networks:
      - higherself-vm-network
    labels:
      - "vm.service=backup"

volumes:
  mongodb_vm_data:
    name: higherself_vm_mongodb_data
    driver: local
  redis_vm_data:
    name: higherself_vm_redis_data
    driver: local
  consul_vm_data:
    name: higherself_vm_consul_data
    driver: local
  prometheus_vm_data:
    name: higherself_vm_prometheus_data
    driver: local
  grafana_vm_data:
    name: higherself_vm_grafana_data
    driver: local

networks:
  higherself-vm-network:
    driver: bridge
    name: higherself-vm-network
    ipam:
      config:
        - subnet: 10.100.0.0/16
    labels:
      - "vm.network=production"
      - "business.entities=multi"
